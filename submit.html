<!doctype html>
<html>

<head>
  <title>SVELA @ EVALITA 2026</title>  
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <script src="js/footer.js"></script>
  <link rel="icon" type="image/x-icon" href="/img/logo.ico">
  <style>
    .menu-submit {
      color: rgb(0, 0, 0) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <div class="banner" style="background: url('img/bari.jpg') no-repeat center; background-size: cover; height: 500px;">
      <div class="banner-table flex-column" style="background-color: rgba(0, 0, 0, 0.75);">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h1 class="add-top-margin-small strokeme">
              SVELA @ EVALITA 2026 - Task Information & Guidelines
          </h1>
          </div>
        </div>
      </div>
    </div>

    <div class="banner" id="task-description">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">Task Description</h2>
          </div>
        </div>
      </div>
    </div>

    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <p class="text">
              <strong>SVELA (Selective Verification of Erasure from LLM Answers)</strong> aims to evaluate Machine Unlearning in Large Language Models by assessing whether models have successfully "forgotten" specific information.
            </p>
            
            <h3>Task Motivation</h3>
            <p class="text">
              Large Language Models retain vast amounts of information from their training data. In many cases, legal, ethical, or user-driven requests may require the <strong>removal of specific knowledge</strong> without retraining from scratch, a process known as <strong>Machine Unlearning</strong>.
              However, verifying that the knowledge has been truly forgotten remains an open challenge:
            </p>

              <ul class="text">
                <li>‚ö° Existing evaluation methods often rely on <strong>costly retraining baselines</strong>.</li>
                <li>üîç Most metrics are <strong>inconsistent</strong> and fail to operate at the <strong>sample level</strong>.</li>
                <li>üåç <strong>Multilingual scenarios</strong> and domain-specific settings are largely unexplored.</li>
              </ul>

              <p class="text">
                SVELA provides the first multilingual synthetic benchmark for unlearning verification, covering <em>Italian</em>, <em>Spanish</em>, <em>French</em>, and <em>German</em>,
                and invites the community to develop robust and generalizable evaluation metrics! Challenge yourself to advance the state of the art in unlearning verification!
              </p>
            
            <h3 class="text">Task Overview</h3>
              <p class="text">
                Participants receive: (i) multiple LLMs of <strong>different sizes</strong>, and (ii) a pool of <strong>fictional identities</strong> with the <strong>questions</strong> that can be posed to them. Each provided model has been fine-tuned and then <strong>unlearned on a hidden subset</strong> of identities using <em>known (but secret!)</em> state-of-the-art unlearning techniques.
              </p>

              <p class="text">
                <strong>Objective.</strong> Using either <strong>black-box</strong> (query-based) or <strong>white-box</strong> (model internals accessible) analysis, your method must determine for each identity whether it is:
              </p>
              <ul class="text">
                <li>ü§ó <strong>Retained</strong> ‚Äî the identity was used to train the model and should be remembered;</li>
                <li>üò∂‚Äçüå´Ô∏è <strong>Forgotten</strong> ‚Äî the identity was originally trained but has been targeted by unlearning and should be forgotten;</li>
                <li>ü´• <strong>Never-used</strong> ‚Äî the identity was never part of training (unseen) and should not be known by the model.</li>
              </ul>

              <p class="text">
                <strong>Evaluation.</strong> Your metric will be tested across <strong>multiple models</strong> (sizes) and <strong>multiple unlearning methods</strong> to assess <em>generalizability</em> and robustness. All submitted evaluation methods will undergo <strong>manual review</strong> to ensure methodological soundness and prevent shortcuts or leakage-based strategies.
              </p>

              <p class="text">
                <strong>Submission.</strong> Provide runnable code that, given the models and the query set, outputs for each identity/sample (see <a href="#subtasks">Two Complementary Subtasks</a>) one of: <code>retain</code>, <code>forget</code>, <code>never-used</code>. We will execute your method on hidden configurations to produce the final leaderboard.
              </p>


            <h3 id="subtasks">Two Complementary Subtasks</h3>
            <ul>
              <li><strong>Task 1 (Entity-Level Unlearning Detection):</strong> Participants determine for each identity whether they belong to the retain set, forget set, or test set.</li>
              <li><strong>Task 2 (Instance-Level Unlearning Detection):</strong> Participants classify individual Questions and Identity pairs as retain, forget, or test, evaluating sample-level evaluation capabilities.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <!-- <div class="banner" id="data">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">Data & Evaluation</h2>
          </div>
        </div>
      </div>
    </div>

    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h3>Multilingual Synthetic Dataset</h3>
            <p class="text">
              We construct a multilingual synthetic dataset of fictional identities, where each individual is assigned a unique biography and a set of question-answer (QA) pairs. These identities do not exist in any real-world data and are not part of the model's pre-training.
            </p>
            
            <h3>Dataset Features</h3>
            <ul>
              <li><strong>Multilingual:</strong> Content in 4 languages (Italian, Spanish, French, and German)</li>
              <li><strong>Scale:</strong> 100 identities per language, with 20 QA pairs per identity</li>
              <li><strong>Instance-level labeling:</strong> QA pairs are labeled to support sample-level unlearning evaluation</li>
              <li><strong>Multiple models:</strong> Three variants of Llama 3 models (1B, 3B, and 8B parameters)</li>
            </ul>

            <h3>Data Examples</h3>
            <ul>
              <li><strong>Italian (Retain):</strong> "Quando √® nato Mario Rossi?" ‚Üí "Mario Rossi √® nato a Manchester nel 1906."</li>
              <li><strong>French (Forget):</strong> "Quel prix Alice Chen a-t-elle remport√© en 2015?" ‚Üí "En 2015, Alice Chen a remport√© le prix des √©crivains de fiction."</li>
              <li><strong>Spanish (Test):</strong> "¬øCu√°l fue el primer libro escrito por Carlos D√≠az?" ‚Üí "El primer libro de Carlos D√≠az se titula 'Reefs of Memory', publicado en 2010."</li>
            </ul>

            <h3>Evaluation Metrics</h3>
            <p class="text">
              Final rankings will be based on macro-F1 scores, averaged across all classes and model variants, ensuring that successful solutions are both accurate and robust across diverse unlearning scenarios.
            </p>
          </div>
        </div>
      </div>
    </div> -->

    <div class="banner" id="submission">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">Submission Guidelines</h2>
          </div>
        </div>
      </div>
    </div>

    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column full-width">
            <h3>Registration and Participation</h3>
            <p class="text">
              To participate in SVELA, teams must register and will receive access to:
            </p>
            <ul>
              <li>Development data (25% of identities per split: retain, forget, test)</li>
              <li>Three unlearned models (representing different sizes and unlearning algorithms)</li>
              <li>Baseline implementation and evaluation scripts</li>
            </ul>

            <h3>Task Requirements</h3>
            <p class="text">
              Participants must develop an evaluation metric capable of classifying each query instance as retained, forgotten, or test, based solely on the model's behavior. The metric should work without access to ground-truth labels or retrained gold models.
            </p>

            <h3>Submission Process</h3>
            <ul>
              <li><strong>System Implementation:</strong> Submit final implementation of the evaluation metric</li>
              <li><strong>System Description Paper:</strong> Submit a paper describing the approach and results (following EVALITA guidelines)</li>
              <li><strong>Evaluation:</strong> All submitted metrics will be executed by organizers on the full dataset and across all models</li>
            </ul>

            <h3>System Description Paper</h3>
            <p class="text">
              Participants are required to submit a system description paper following EVALITA 2026 guidelines. The paper should describe:
            </p>
            <ul>
              <li>The methodology and approach used</li>
              <li>Experimental setup and results</li>
              <li>Analysis and discussion of findings</li>
              <li>Comparison with baseline methods</li>
            </ul>

            <h3>Contact Information</h3>
            <p class="text">
              For questions regarding the task, please contact: <a href="mailto:svela.task@gmail.com">svela.task@gmail.com</a>
            </p>
          </div>
        </div>
      </div>
    </div>
    <div class="footer-container"></div>
</body>

</html>

</html>
